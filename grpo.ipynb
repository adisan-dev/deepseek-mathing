{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softmax(self.network(x), dim=1)\n",
    "\n",
    "    def get_action(self, state: List[float]) -> Tuple[int, float]:\n",
    "        \"\"\"Return action and its probability\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(state_tensor)\n",
    "\n",
    "        # Sample action\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        action_prob = probs[0, action].item()\n",
    "\n",
    "        return action.item(), action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state: np.array, action: int, reward: float, prob: float, done: bool):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.probs.append(prob)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def _compute_returns(self, gamma: float = 0.99) -> List[float]:\n",
    "        returns = []\n",
    "        G = 0\n",
    "\n",
    "        for r, d in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            G = r + gamma * G * (1 - int(d))\n",
    "            returns.append(G)\n",
    "\n",
    "        return list(reversed(returns))\n",
    "\n",
    "    def get_return(self, gamma: float) -> float:\n",
    "        \"\"\"Returns return (not reward) for the entire trajectory\"\"\"\n",
    "        returns = self._compute_returns(gamma)\n",
    "        # If you do sum, numbers would go wild based on length of the game when _compute_returns is applied\n",
    "        # Mean is larger if game was larger, but the differences aren't wild like when summing\n",
    "        return np.mean(returns)\n",
    "\n",
    "    def to_tensor(self, gamma: float = 0.99):\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        actions = torch.LongTensor(np.array(self.actions))\n",
    "        probs = torch.stack([torch.tensor(p) for p in self.probs])\n",
    "        returns = torch.FloatTensor(self._compute_returns(gamma))\n",
    "\n",
    "        return states, actions, probs, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.9403989999999998, 2.9701, 1.99, 1.0] 2.47512475\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "t = Trajectory()\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, prob=0.2, done=True)\n",
    "\n",
    "gamma = 0.99\n",
    "print(t._compute_returns(gamma), t.get_return(gamma))\n",
    "\n",
    "states, actions, probs, returns = t.to_tensor()\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRPO:\n",
    "    def __init__(self, env: gym.Env, hidden_dim: int = 64, lr_policy: float = 1e-3, gamma: float = 0.99, n_groups: int = 3, clip_param: float = 0.2):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.n_groups = n_groups\n",
    "        self.clip_param = clip_param\n",
    "\n",
    "        self.policy_net = PolicyNetwork(input_dim=self.state_dim, hidden_dim=hidden_dim, output_dim=self.action_dim)\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=lr_policy)\n",
    "\n",
    "    def collect_trajectories(self, n_trajectories: int):\n",
    "        trajectories = []\n",
    "\n",
    "        for _ in range(n_trajectories):\n",
    "            traj = Trajectory()\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, prob = self.policy_net.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                traj.add(state=state, action=action, reward=reward, prob=prob, done=done)\n",
    "                state = next_state\n",
    "\n",
    "            trajectories.append(traj)\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def group_trajectories(self, trajectories: List[Trajectory]):\n",
    "        \"\"\"Group trajectories into `self.n_groups` for comparisons\"\"\"\n",
    "        # traj_with_returns = [(traj, traj.get_return(self.gamma)) for traj in trajectories]\n",
    "\n",
    "        traj_with_returns = []\n",
    "\n",
    "        for traj in trajectories:\n",
    "            mean_returns = traj.get_return(self.gamma) # rewards and returns are different\n",
    "            traj_with_returns.append((traj, mean_returns))\n",
    "\n",
    "        sorted_trajectories = [t for (t, _) in sorted(traj_with_returns, key=lambda x: x[1])] # ascending order of rewards\n",
    "\n",
    "        grouped_trajectories = []\n",
    "        group_size = max(1, len(sorted_trajectories) // self.n_groups)\n",
    "\n",
    "        for i in range(0, len(sorted_trajectories), group_size):\n",
    "            group = sorted_trajectories[i:i + group_size]\n",
    "            if len(group) > 0:\n",
    "                grouped_trajectories.append(group)\n",
    "\n",
    "        # Ensure we don't have more than n_groups by merging groups (starting from the end - i.e. highest returns)\n",
    "        while len(grouped_trajectories) > self.n_groups:\n",
    "            if len(grouped_trajectories) >= 2:\n",
    "                grouped_trajectories[-2].extend(grouped_trajectories[-1])\n",
    "                grouped_trajectories.pop()\n",
    "\n",
    "        return grouped_trajectories\n",
    "\n",
    "    def update_policy(self, grouped_trajectories: List[List[Trajectory]]):\n",
    "        \"\"\"Updated policy using group relative approach\"\"\"\n",
    "        for group_idx, group in enumerate(grouped_trajectories):\n",
    "            # Group weight scales with group index: because we sorted them in ascending order by scores\n",
    "            group_weight = (group_idx + 1) / len(grouped_trajectories)\n",
    "\n",
    "            for trajectory in group:\n",
    "                states, actions, old_probs, returns = trajectory.to_tensor(self.gamma)\n",
    "\n",
    "                if len(states) == 0:\n",
    "                    continue\n",
    "\n",
    "                current_probs = self.policy_net(states)\n",
    "                dist = Categorical(current_probs)\n",
    "\n",
    "                # Get log prob for the action take\n",
    "                log_probs = dist.log_prob(actions)\n",
    "                old_log_probs = torch.log(old_probs + 1e-10) # add small epsilon to avoid log(0)\n",
    "\n",
    "                ratios = torch.exp(log_probs - old_log_probs)\n",
    "                surr1 = ratios * returns * group_weight\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_param, 1 + self.clip_param) * returns * group_weight\n",
    "                # print(\"Surr losses\", surr1.mean().item(), \"ratio\", ratios.mean().item(), \"returns\", returns.mean().item(), \"group weight\", group_weight)\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "    def train(self, n_episodes: int, n_trajectories_per_update: int = 10):\n",
    "        rewards_history = []\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            trajectories = self.collect_trajectories(n_trajectories_per_update)\n",
    "\n",
    "            # Just for logging:\n",
    "            avg_reward = np.mean([sum(traj.rewards) for traj in trajectories])\n",
    "            rewards_history.append(avg_reward)\n",
    "\n",
    "            grouped_trajectories = self.group_trajectories(trajectories)\n",
    "\n",
    "            self.update_policy(grouped_trajectories)\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(\"Episode {}, Avg reward: {:.2f}\".format(episode+1, avg_reward))\n",
    "\n",
    "        return rewards_history\n",
    "\n",
    "    def evaluate(self, env: gym.Env, n_episodes: int = 10, render: bool = False):\n",
    "        rewards = []\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                action, _ = self.policy_net.get_action(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(\"Evaluation: Average reward over {} episodes: {:.2f}\".format(n_episodes, avg_reward))\n",
    "\n",
    "        return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init grpo algorithm and env\n",
    "\n",
    "ENV_NAME = 'CartPole-v1' # Possible values: CartPole-v1, Acrobot-v1\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
    "grpo = GRPO(env=env, hidden_dim=64, lr_policy=0.001, gamma=0.99, n_groups=3, clip_param=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Avg reward: 29.60\n",
      "Episode 20, Avg reward: 29.40\n",
      "Episode 30, Avg reward: 40.20\n",
      "Episode 40, Avg reward: 100.40\n",
      "Episode 50, Avg reward: 72.20\n",
      "Episode 60, Avg reward: 72.60\n",
      "Episode 70, Avg reward: 192.80\n",
      "Episode 80, Avg reward: 103.00\n",
      "Episode 90, Avg reward: 313.40\n",
      "Episode 100, Avg reward: 341.60\n",
      "Episode 110, Avg reward: 171.80\n",
      "Episode 120, Avg reward: 311.80\n",
      "Episode 130, Avg reward: 500.00\n",
      "Episode 140, Avg reward: 317.80\n",
      "Episode 150, Avg reward: 192.00\n",
      "Episode 160, Avg reward: 164.60\n",
      "Episode 170, Avg reward: 317.80\n",
      "Episode 180, Avg reward: 486.40\n",
      "Episode 190, Avg reward: 500.00\n",
      "Episode 200, Avg reward: 500.00\n",
      "Episode 210, Avg reward: 500.00\n",
      "Episode 220, Avg reward: 476.00\n",
      "Episode 230, Avg reward: 500.00\n",
      "Episode 240, Avg reward: 479.20\n",
      "Episode 250, Avg reward: 500.00\n",
      "Episode 260, Avg reward: 500.00\n",
      "Episode 270, Avg reward: 485.40\n",
      "Episode 280, Avg reward: 500.00\n",
      "Episode 290, Avg reward: 500.00\n",
      "Episode 300, Avg reward: 500.00\n",
      "Evaluation: Average reward over 10 episodes: 500.00\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "# Can be rerun multiple times to improve performance of the same model\n",
    "rewards = grpo.train(n_episodes=300, n_trajectories_per_update=5)\n",
    "avg_reward = grpo.evaluate(env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Average reward over 1 episodes: 500.00\n"
     ]
    }
   ],
   "source": [
    "## Evaluate:\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500, render_mode='human')\n",
    "_ = grpo.evaluate(env, 1, True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
